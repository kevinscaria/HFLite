{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dimensions (L = length of sequence | d_k, d_q and d_v are the dimension of the query, key and value vector respectively)\n",
    "L, d_q, d_k, d_v = 4, 8, 8, 8\n",
    "\n",
    "q = np.random.rand(L, d_q)\n",
    "k = np.random.rand(L, d_k)\n",
    "v = np.random.rand(L, d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.97034639, 2.09695661, 2.28891774, 2.13474498],\n",
       "       [1.68359999, 1.99113588, 1.76012616, 2.03409332],\n",
       "       [1.35021503, 1.24722467, 0.91013416, 1.0667486 ],\n",
       "       [1.76674198, 2.18895891, 2.24594791, 2.26042321]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# K.Q^T\n",
    "np.matmul(q, k.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.69662265, 0.74138612, 0.80925463, 0.75474632],\n",
       "       [0.59524249, 0.70397284, 0.62229857, 0.71916059],\n",
       "       [0.4773731 , 0.44096051, 0.32178102, 0.37715259],\n",
       "       [0.62463762, 0.77391384, 0.7940625 , 0.79918029]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why do we need the sqrt(d_k) --> To stabilize the variance of the dot-product of query and key values\n",
    "q.var(), k.var(), np.matmul(q, k.T).var(), (np.matmul(q, k.T)/np.sqrt(d_k)).var()\n",
    "\n",
    "# Scaled dot-product\n",
    "scaled_dot_product = np.matmul(q, k.T)/np.sqrt(d_k)\n",
    "scaled_dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.69662265,       -inf,       -inf,       -inf],\n",
       "       [0.59524249, 0.70397284,       -inf,       -inf],\n",
       "       [0.4773731 , 0.44096051, 0.32178102,       -inf],\n",
       "       [0.62463762, 0.77391384, 0.7940625 , 0.79918029]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Masking - To ensure we do not look ahead for decoding\n",
    "mask = np.tril(np.ones((L, L)))\n",
    "mask[mask == 0] = -np.infty\n",
    "mask[mask == 1] = 0\n",
    "\n",
    "scaled_dot_product = scaled_dot_product + mask\n",
    "scaled_dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.90358946, 0.52715584, 0.        , 0.        ],\n",
       "       [0.80312128, 0.40524178, 0.30349735, 0.        ],\n",
       "       [0.93054482, 0.5653456 , 0.48670314, 0.26248191]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attention (After Softmax)\n",
    "def softmax(x):\n",
    "    return np.exp(scaled_dot_product)/np.sum(np.exp(scaled_dot_product), axis=-1)\n",
    "\n",
    "attention = softmax(scaled_dot_product)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97760902, 0.422824  , 0.23773325, 0.25334508, 0.01667512,\n",
       "        0.29083252, 0.38963413, 0.58275223],\n",
       "       [1.33535946, 0.69665477, 0.22186223, 0.55445162, 0.26947833,\n",
       "        0.42079728, 0.46213142, 0.59249975],\n",
       "       [1.41975113, 0.66866223, 0.49369465, 0.75235602, 0.36916941,\n",
       "        0.43552976, 0.61813096, 0.79486238],\n",
       "       [1.90960847, 1.07926621, 0.90911764, 1.1603667 , 0.59701321,\n",
       "        0.63747457, 0.91029532, 1.13494983]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_scores = np.matmul(attention, v)\n",
    "attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encapsulating the scaled_dot_product attention\n",
    "def scaled_dot_product_attention(q, k, v, mask = None):\n",
    "    d_k = q.shape[-1]\n",
    "    scaled_dot_product = np.matmul(q, k.T)/np.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled_dot_product = scaled_dot_product + mask\n",
    "    attention = softmax(scaled_dot_product)\n",
    "    attention_scores = np.matmul(attention, v)\n",
    "    return attention_scores, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.90358946, 0.52715584, 0.        , 0.        ],\n",
       "       [0.80312128, 0.40524178, 0.30349735, 0.        ],\n",
       "       [0.93054482, 0.5653456 , 0.48670314, 0.26248191]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values, attention_matrix = scaled_dot_product_attention(q, k, v, mask)\n",
    "attention_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length = 4\n",
    "batch_size = 1\n",
    "input_dim = 512\n",
    "d_model = 512\n",
    "x = torch.randn((batch_size, sequence_length, input_dim))\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 1536])\n",
      "torch.Size([1, 4, 8, 192])\n",
      "torch.Size([1, 8, 4, 192])\n",
      "torch.Size([1, 8, 4, 64]) torch.Size([1, 8, 4, 64]) torch.Size([1, 8, 4, 64])\n"
     ]
    }
   ],
   "source": [
    "# Scaled Dot Product in Multi Head Scenario\n",
    "qkv_layer = nn.Linear(input_dim, 3 * d_model)\n",
    "\n",
    "qkv = qkv_layer(x)\n",
    "print(qkv.size())\n",
    "\n",
    "num_heads = 8\n",
    "head_dim = d_model//8\n",
    "qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3 * head_dim)\n",
    "print(qkv.size())\n",
    "\n",
    "# Permute to get tensor (batch size, num_heads, sequence_length, d_model)\n",
    "qkv = qkv.permute(0, 2, 1, 3)\n",
    "print(qkv.size())\n",
    "\n",
    "q, k, v = qkv.chunk(3, dim=-1)\n",
    "print(q.size(), k.size(), v.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 4, 64])\n",
      "torch.Size([1, 8, 4, 4])\n",
      "torch.Size([1, 8, 4, 64])\n",
      "torch.Size([1, 8, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "def torch_scaled_dot_product_attention(q, k, v, mask = None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled_dot_product = torch.matmul(q, torch.transpose(k, -2, -1))/math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled_dot_product += mask\n",
    "    attention = torch.softmax(scaled_dot_product, -1)\n",
    "    attention_scores = torch.matmul(attention, v)\n",
    "    return attention_scores, attention\n",
    "\n",
    "# For encoders\n",
    "att_score, att_matrix = torch_scaled_dot_product_attention(q, k, v)\n",
    "print(att_score.size())\n",
    "print(att_matrix.size())\n",
    "\n",
    "#  For decoders\n",
    "mask = torch.full((batch_size, num_heads, sequence_length, sequence_length), float(\"-inf\"))\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "att_score, att_matrix = torch_scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "print(att_score.size())\n",
    "print(att_matrix.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 5, 1024])\n",
      "torch.Size([30, 5, 1536])\n",
      "torch.Size([30, 5, 8, 192])\n",
      "torch.Size([30, 8, 5, 192])\n",
      "torch.Size([30, 8, 5, 64]) torch.Size([30, 8, 5, 64]) torch.Size([30, 8, 5, 64])\n",
      "torch.Size([30, 8, 5, 64])\n",
      "torch.Size([30, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model//num_heads\n",
    "        self.qkv_layer = nn.Linear(self.input_dim, 3 * d_model)\n",
    "        self.feed_forward = nn.Linear(d_model, d_model)\n",
    "\n",
    "    @staticmethod\n",
    "    def scaled_dot_product_attention(q, k, v, mask = None):\n",
    "        d_k = q.size()[-1]\n",
    "        scaled_dot_product = torch.matmul(q, torch.transpose(k, -2, -1))/math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            scaled_dot_product += mask\n",
    "        attention = torch.softmax(scaled_dot_product, -1)\n",
    "        attention_scores = torch.matmul(attention, v)\n",
    "        return attention_scores, attention\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Input tensor (batch_size, sequence_length, input_dim)\n",
    "        batch_size, sequence_length = x.size()[0], x.size()[1]\n",
    "        qkv_tensor = self.qkv_layer(x)\n",
    "        print(qkv_tensor.size())\n",
    "\n",
    "        # Reshape to accomodate heads\n",
    "        qkv_tensor = qkv_tensor.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "        print(qkv_tensor.size())\n",
    "\n",
    "        # Permute to get tensor (batch size, num_heads, sequence_length, d_model)\n",
    "        qkv_tensor = qkv_tensor.permute(0, 2, 1, 3)\n",
    "        print(qkv_tensor.size())\n",
    "\n",
    "        # Extract individual q, k and v\n",
    "        q, k, v = qkv_tensor.chunk(3, dim=-1)\n",
    "        print(q.size(), k.size(), v.size())\n",
    "\n",
    "        # Compute attention\n",
    "        attention_score, self_attention_matrix = MultiHeadAttention.scaled_dot_product_attention(q, k, v)\n",
    "        print(attention_score.size())\n",
    "\n",
    "        # Gather the heads\n",
    "        attention_score = attention_score.reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
    "        print(attention_score.size())\n",
    "\n",
    "        # Layer adjustnemt\n",
    "        encoded_input = self.feed_forward(attention_score)\n",
    "\n",
    "        return encoded_input\n",
    "    \n",
    "# Test it out\n",
    "input_dim = 1024\n",
    "sequence_length = 5\n",
    "batch_size = 30\n",
    "\n",
    "input_sequence = torch.randn((batch_size, sequence_length, input_dim))\n",
    "print(input_sequence.size())\n",
    "\n",
    "attention_block = MultiHeadAttention(\n",
    "    input_dim=input_dim,\n",
    "    d_model=512, \n",
    "    num_heads=8\n",
    "    )\n",
    "value = attention_block.forward(input_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 5, 512])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
